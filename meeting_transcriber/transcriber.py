# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Whisper.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç faster-whisper, openai-whisper –∏ whisperx (—Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π) backends.
"""

import os
import time
import json
import datetime
import platform
import subprocess
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple

from .config import Config
from .utils import ffprobe_ok, get_audio_duration, format_timestamp_srt
from .logging_setup import get_logger

logger = get_logger()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å WhisperX
HAS_WHISPERX = False
try:
    from .whisperx import WhisperXTranscriber, check_whisperx_available
    HAS_WHISPERX = check_whisperx_available()
except ImportError:
    pass

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ torch
HAS_TORCH = False
try:
    import torch
    HAS_TORCH = True
except ImportError:
    pass

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ tqdm
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable=None, **kwargs):
        logger.warning("tqdm –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tqdm")
        return iterable if iterable is not None else []


class EnhancedTranscriber:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç faster-whisper, openai-whisper –∏ whisperx (—Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π) backends.
    """
    
    def __init__(
        self,
        diarize: bool = False,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä–∞.
        
        Args:
            diarize: –í–∫–ª—é—á–∏—Ç—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤ (—Ç—Ä–µ–±—É–µ—Ç whisperx backend)
            min_speakers: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (hint –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏)
            max_speakers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (hint –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏)
        """
        Config.ensure_directories()
        self.model = None
        self.model_loaded = False
        self.model_size = Config.DEFAULT_MODEL
        self.backend = Config.ASR_BACKEND
        self.device = 'cpu'
        self.use_fp16 = False
        self.diarize = diarize
        self.min_speakers = min_speakers
        self.max_speakers = max_speakers
        
        # WhisperX —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä (–¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏)
        self.whisperx_transcriber = None
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ whisperx –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        if diarize and self.backend != 'whisperx':
            if HAS_WHISPERX:
                logger.info("–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ whisperx backend")
                self.backend = 'whisperx'
            else:
                logger.warning(
                    "–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ whisperx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install whisperx. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞."
                )
                self.diarize = False  # –û—Ç–∫–ª—é—á–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ

    def _resolve_device_whisper(self) -> Tuple[str, bool]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è openai-whisper."""
        d = Config.ASR_DEVICE
        
        if d == 'auto':
            if HAS_TORCH and torch.cuda.is_available():
                return 'cuda', True
            if HAS_TORCH and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return 'mps', False
            return 'cpu', False
        
        if d == 'cuda':
            if HAS_TORCH and torch.cuda.is_available():
                return 'cuda', True
            return 'cpu', False
        
        if d in ('mps', 'metal'):
            ok = HAS_TORCH and hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
            return ('mps', False) if ok else ('cpu', False)
        
        return 'cpu', False

    def _resolve_device_faster(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è faster-whisper."""
        d = Config.ASR_DEVICE
        if d in ('auto', 'cpu', 'cuda', 'metal'):
            return d
        if d == 'mps':
            return 'metal'
        return 'auto'

    def _load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Whisper."""
        if self.model_loaded:
            logger.debug("–ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return
        
        logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ '{self.model_size}' (backend={self.backend})...")
        load_start = time.time()
        
        if self.backend == 'whisperx':
            # WhisperX —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
            if not HAS_WHISPERX:
                raise ImportError(
                    "whisperx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install whisperx"
                )
            self.whisperx_transcriber = WhisperXTranscriber()
            self.whisperx_transcriber.load_model()
            self.device = self.whisperx_transcriber.device
        
        elif self.backend == 'faster':
            from faster_whisper import WhisperModel
            
            device = self._resolve_device_faster()
            cpu_threads = Config.FASTER_CPU_THREADS if device == 'cpu' else 0
            
            logger.debug(
                f"faster-whisper: device={device}, "
                f"compute_type={Config.FASTER_COMPUTE}, "
                f"cpu_threads={cpu_threads}"
            )
            
            self.model = WhisperModel(
                self.model_size,
                device=device,
                compute_type=Config.FASTER_COMPUTE,
                cpu_threads=cpu_threads
            )
            self.device = device
        else:
            import whisper
            
            device, fp16 = self._resolve_device_whisper()
            logger.debug(f"openai-whisper: device={device}, fp16={fp16}")
            
            self.model = whisper.load_model(self.model_size, device=device)
            self.device = device
            self.use_fp16 = fp16
        
        load_time = time.time() - load_start
        self.model_loaded = True
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (device={self.device}) –∑–∞ {load_time:.1f} —Å–µ–∫")

    def transcribe_files(self, files: List[Path]) -> None:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤.
        
        Args:
            files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞–º
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é {len(files)} —Ñ–∞–π–ª(–æ–≤)")
        self._load_model()
        
        success = 0
        total = len(files)
        
        for i, f in enumerate(files, 1):
            print(f"\n‚îÅ‚îÅ‚îÅ –§–∞–π–ª {i}/{total}: {f.name} ‚îÅ‚îÅ‚îÅ")
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i}/{total}: {f.name}")
            
            ok = self._transcribe_single(f, auto_open=(i == 1))
            success += 1 if ok else 0
        
        logger.info(f"üìä –ò—Ç–æ–≥: —É—Å–ø–µ—à–Ω–æ {success}/{total}, –æ—à–∏–±–æ–∫ {total - success}")
        print(f"\nüìä –ò—Ç–æ–≥: —É—Å–ø–µ—à–Ω–æ {success}/{total}, –æ—à–∏–±–æ–∫ {total - success}")

    def _transcribe_single(self, audio_file: Path, auto_open: bool = True) -> bool:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª.
        
        Args:
            audio_file: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            auto_open: –û—Ç–∫—Ä—ã—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            
        Returns:
            True –ø—Ä–∏ —É—Å–ø–µ—Ö–µ, False –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not audio_file.exists():
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file}")
            return False
        
        if not ffprobe_ok(audio_file):
            logger.error(f"–§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∞—É–¥–∏–æ: {audio_file}")
            return False
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ safe WAV (16kHz, mono)
        safe_file = audio_file.with_suffix(
            f".safe{datetime.datetime.now():%H%M%S}.wav"
        )
        
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ 16kHz mono WAV)...")
        print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ...")
        
        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", str(audio_file),
                "-ar", "16000", "-ac", "1", "-c:a", "pcm_s16le", "-nostdin",
                str(safe_file)
            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            logger.debug(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {safe_file}")
        except subprocess.CalledProcessError as e:
            logger.error(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            return False
        
        t0 = time.time()
        language = 'ru' if Config.FORCE_RU else None
        
        try:
            # WhisperX backend (—Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π)
            if self.backend == 'whisperx':
                result = self._run_whisperx(safe_file, language=language)
            else:
                # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ ‚Äî –ë–ï–ó VAD
                logger.debug(f"ASR –ø—Ä–æ—Ö–æ–¥ 1: language={language}, vad=off")
                result = self._run_asr_once(safe_file, language=language, use_vad=False)
                
                # Fallback ‚Äî —Å VAD –∏ ru
                if not result or not result.get("segments"):
                    logger.warning("–ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ –ø—É—Å—Ç, –ø—Ä–æ–±—É–µ–º —Å VAD...")
                    print("‚ö†Ô∏è –ü—É—Å—Ç–æ –±–µ–∑ VAD, –ø—Ä–æ–±—É—é —Å VAD...")
                    result = self._run_asr_once(
                        safe_file,
                        language=language or 'ru',
                        use_vad=True
                    )
            
            if not result or not result.get("text", "").strip():
                logger.error("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                return False
            
            elapsed = time.time() - t0
            word_count = len(result['text'].split())
            segment_count = len(result['segments'])
            speakers = result.get('speakers', [])
            
            if speakers:
                logger.info(
                    f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {segment_count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, "
                    f"{word_count} —Å–ª–æ–≤, {len(speakers)} —Å–ø–∏–∫–µ—Ä(–æ–≤), {elapsed / 60:.1f} –º–∏–Ω"
                )
                print(f"‚úÖ –°–µ–≥–º–µ–Ω—Ç–æ–≤: {segment_count}, —Å–ª–æ–≤: {word_count}, "
                      f"—Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers)}, –≤—Ä–µ–º—è: {elapsed / 60:.1f} –º–∏–Ω.")
            else:
                logger.info(
                    f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {segment_count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, "
                    f"{word_count} —Å–ª–æ–≤, {elapsed / 60:.1f} –º–∏–Ω"
                )
                print(f"‚úÖ –°–µ–≥–º–µ–Ω—Ç–æ–≤: {segment_count}, —Å–ª–æ–≤: {word_count}, –≤—Ä–µ–º—è: {elapsed / 60:.1f} –º–∏–Ω.")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            base = f"transcript_{audio_file.stem}_{ts}"
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–∏–∫–µ—Ä—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
            has_speakers = speakers or any(
                seg.get('speaker') for seg in result['segments']
            )
            
            if has_speakers:
                txt = self._save_txt_diarized(result, base)
            else:
                txt = self._save_txt(result, base)
            
            jsn = self._save_json(result, base, audio_file.name, language or 'auto')
            srt = self._save_srt(result, base, include_speaker=has_speakers)
            
            logger.info(f"üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {txt.name}, {jsn.name}, {srt.name}")
            print("üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:", txt.name, jsn.name, srt.name)
            
            if auto_open:
                self._open_file(txt)
            
            return True
        finally:
            if safe_file.exists():
                try:
                    safe_file.unlink()
                    logger.debug("–í—Ä–µ–º–µ–Ω–Ω—ã–π safe WAV —Ñ–∞–π–ª —É–¥–∞–ª—ë–Ω")
                except OSError as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {e}")

    def _run_asr_once(
        self,
        wav_file: Path,
        language: Optional[str],
        use_vad: bool
    ) -> Optional[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ ASR.
        
        Args:
            wav_file: –ü—É—Ç—å –∫ WAV —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ –∏–ª–∏ None –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            use_vad: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Voice Activity Detection
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å text –∏ segments –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        total_sec = get_audio_duration(wav_file)
        logger.debug(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: {total_sec:.1f} —Å–µ–∫")
        
        pbar = tqdm(
            total=int(total_sec) if total_sec > 0 else None,
            desc="–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è",
            unit="s",
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
        )
        
        segs: List[Dict] = []
        texts: List[str] = []
        last_progress = 0
        last_print = time.time()
        
        try:
            logger.info(f"ASR start (lang={language}, vad={'on' if use_vad else 'off'})")
            print(f" > ASR start (lang={language}, vad={'on' if use_vad else 'off'})")
            
            if self.backend == 'faster':
                segments_it, info = self.model.transcribe(
                    str(wav_file),
                    language=language,
                    vad_filter=use_vad,
                    beam_size=Config.FASTER_BEAM_SIZE,
                    word_timestamps=True
                )
                
                for s in segments_it:
                    segs.append({
                        'start': s.start,
                        'end': s.end,
                        'text': s.text
                    })
                    texts.append(s.text)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                    if total_sec and s.end is not None:
                        cur = int(s.end)
                        if cur > last_progress:
                            pbar.update(cur - last_progress)
                            last_progress = cur
                    
                    # –ñ–∏–≤–æ–π —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥—ã–µ ~2.5 —Å–µ–∫
                    if time.time() - last_print > 2.5:
                        mm = int(s.end) // 60 if s.end else 0
                        ss = int(s.end) % 60 if s.end else 0
                        print(f"\r‚Ä¶ t‚âà{mm:02d}:{ss:02d}, —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segs)}", 
                              end="", flush=True)
                        last_print = time.time()
                    
                    if Config.DEBUG_SEGMENTS:
                        logger.debug(f"[{s.start:.2f}-{s.end:.2f}] {s.text[:60]}")
                
                if language is None:
                    language = getattr(info, 'language', None)
                    logger.debug(f"–û–ø—Ä–µ–¥–µ–ª—ë–Ω —è–∑—ã–∫: {language}")
            
            else:  # openai/whisper
                import whisper
                
                res = self.model.transcribe(
                    str(wav_file),
                    language=language,
                    fp16=self.use_fp16,
                    word_timestamps=True
                )
                segs = res.get("segments", [])
                texts = [seg.get("text", "") for seg in segs]
                pbar.update(int(total_sec) if total_sec else 0)
            
            logger.debug(f"ASR –∑–∞–≤–µ—Ä—à—ë–Ω: {len(segs)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            return {'text': " ".join(texts).strip(), 'segments': segs}
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ ASR: {e}", exc_info=True)
            raise
        finally:
            if total_sec and pbar.n < int(total_sec):
                pbar.update(int(total_sec) - pbar.n)
            pbar.close()
            print()  # –ü–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

    def _run_whisperx(
        self,
        wav_file: Path,
        language: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —á–µ—Ä–µ–∑ WhisperX —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π.
        
        Args:
            wav_file: –ü—É—Ç—å –∫ WAV —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ –∏–ª–∏ None –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å text, segments, speakers
        """
        if not self.whisperx_transcriber:
            raise RuntimeError("WhisperX —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        return self.whisperx_transcriber.transcribe(
            wav_file,
            language=language,
            diarize=self.diarize,
            min_speakers=self.min_speakers,
            max_speakers=self.max_speakers
        )

    def _save_txt(self, result: Dict, base: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ TXT."""
        p = Config.TRANSCRIPTS_FOLDER / f"{base}.txt"
        with open(p, 'w', encoding='utf-8') as f:
            f.write(result["text"])
        return p

    def _save_txt_diarized(self, result: Dict, base: str) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π –≤ TXT."""
        p = Config.TRANSCRIPTS_FOLDER / f"{base}.txt"
        
        segments = result.get("segments", [])
        lines = []
        current_speaker = None
        current_text = []
        current_start = None
        
        for seg in segments:
            speaker = seg.get("speaker", "SPEAKER")
            text = seg.get("text", "").strip()
            start = seg.get("start", 0)
            
            if not text:
                continue
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            if speaker == current_speaker:
                current_text.append(text)
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –≥—Ä—É–ø–ø—É
                if current_text:
                    ts = self._format_time_short(current_start)
                    combined = " ".join(current_text)
                    lines.append(f"[{ts}] {current_speaker}:\n{combined}")
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É
                current_speaker = speaker
                current_text = [text]
                current_start = start
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥—Ä—É–ø–ø—É
        if current_text:
            ts = self._format_time_short(current_start)
            combined = " ".join(current_text)
            lines.append(f"[{ts}] {current_speaker}:\n{combined}")
        
        with open(p, 'w', encoding='utf-8') as f:
            f.write("\n\n".join(lines))
        
        return p

    @staticmethod
    def _format_time_short(seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –≤ MM:SS."""
        minutes = int(seconds) // 60
        secs = int(seconds) % 60
        return f"{minutes:02d}:{secs:02d}"

    def _save_json(
        self,
        result: Dict,
        base: str,
        audio_name: str,
        language: str
    ) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON."""
        p = Config.TRANSCRIPTS_FOLDER / f"{base}.json"
        data = {
            'timestamp': datetime.datetime.now().isoformat(),
            'audio_file': audio_name,
            'language': language,
            'text': result['text'],
            'segments': result['segments']
        }
        with open(p, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return p

    def _save_srt(
        self,
        result: Dict,
        base: str,
        include_speaker: bool = False
    ) -> Path:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ SRT (—Å—É–±—Ç–∏—Ç—Ä—ã).
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            base: –ë–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            include_speaker: –î–æ–±–∞–≤–ª—è—Ç—å –º–µ—Ç–∫—É —Å–ø–∏–∫–µ—Ä–∞
        """
        p = Config.TRANSCRIPTS_FOLDER / f"{base}.srt"
        with open(p, 'w', encoding='utf-8') as f:
            for i, s in enumerate(result['segments'], 1):
                start = format_timestamp_srt(s.get('start', 0.0))
                end = format_timestamp_srt(s.get('end', 0.0))
                text = (s.get('text') or '').strip()
                
                if include_speaker and s.get('speaker'):
                    text = f"[{s['speaker']}] {text}"
                
                f.write(f"{i}\n{start} --> {end}\n{text}\n\n")
        return p

    def _open_file(self, path: Path) -> None:
        """–û—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏."""
        try:
            logger.debug(f"–û—Ç–∫—Ä—ã–≤–∞—é —Ñ–∞–π–ª: {path}")
            system = platform.system()
            
            if system == "Darwin":
                subprocess.run(["open", str(path)], check=False)
            elif system == "Windows":
                os.startfile(str(path))  # type: ignore
            else:
                subprocess.run(["xdg-open", str(path)], check=False)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª {path}: {e}")

