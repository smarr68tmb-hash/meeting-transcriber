# -*- coding: utf-8 -*-
"""
WhisperX backend —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤.

WhisperX –¥–æ–±–∞–≤–ª—è–µ—Ç:
- –ë—ã—Å—Ç—Ä—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é (faster-whisper)
- –¢–æ—á–Ω—ã–µ word-level timestamps (forced alignment)
- –î–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤ (pyannote.audio)

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- pip install whisperx
- HuggingFace —Ç–æ–∫–µ–Ω –¥–ª—è pyannote (env: HF_TOKEN)
  –ù—É–∂–Ω–æ –ø—Ä–∏–Ω—è—Ç—å –ª–∏—Ü–µ–Ω–∑–∏—é: https://huggingface.co/pyannote/speaker-diarization-3.1
"""

import os
import time
from pathlib import Path
from typing import Optional, Dict, Any, List

from .config import Config
from .logging_setup import get_logger

logger = get_logger()


# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å whisperx
HAS_WHISPERX = False
try:
    import whisperx
    HAS_WHISPERX = True
except ImportError:
    pass

# –ü—Ä–æ–≤–µ—Ä—è–µ–º torch
HAS_TORCH = False
try:
    import torch
    HAS_TORCH = True
except ImportError:
    pass


class WhisperXTranscriber:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä –Ω–∞ –±–∞–∑–µ WhisperX —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π —Å–ø–∏–∫–µ—Ä–æ–≤.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - faster-whisper –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    - wav2vec2 –¥–ª—è forced alignment (—Ç–æ—á–Ω—ã–µ timestamps)
    - pyannote –¥–ª—è speaker diarization
    """
    
    def __init__(self):
        if not HAS_WHISPERX:
            raise ImportError(
                "whisperx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install whisperx"
            )
        
        self.model = None
        self.model_loaded = False
        self.model_size = Config.DEFAULT_MODEL
        self.device = self._resolve_device()
        # float16 –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU, –∏—Å–ø–æ–ª—å–∑—É–µ–º int8
        self.compute_type = Config.WHISPERX_COMPUTE if self.device != 'cpu' else 'int8'
        self.hf_token = Config.HF_TOKEN
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        self._align_model = None
        self._align_metadata = None
        self._align_language = None
    
    def _resolve_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è WhisperX."""
        d = Config.ASR_DEVICE
        
        if d == 'auto':
            if HAS_TORCH and torch.cuda.is_available():
                return 'cuda'
            # WhisperX –Ω–∞ CPU —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ
            return 'cpu'
        
        if d == 'cuda':
            if HAS_TORCH and torch.cuda.is_available():
                return 'cuda'
            logger.warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é CPU")
            return 'cpu'
        
        if d in ('mps', 'metal'):
            # WhisperX –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É MPS
            logger.warning("WhisperX –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É MPS, –∏—Å–ø–æ–ª—å–∑—É—é CPU")
            return 'cpu'
        
        return 'cpu'
    
    def load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å WhisperX."""
        if self.model_loaded:
            logger.debug("–ú–æ–¥–µ–ª—å WhisperX —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return
        
        logger.info(
            f"ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ WhisperX –º–æ–¥–µ–ª–∏ '{self.model_size}' "
            f"(device={self.device}, compute={self.compute_type})..."
        )
        
        load_start = time.time()
        
        self.model = whisperx.load_model(
            self.model_size,
            device=self.device,
            compute_type=self.compute_type,
            language=Config.WHISPERX_LANGUAGE if Config.FORCE_RU else None
        )
        
        load_time = time.time() - load_start
        self.model_loaded = True
        logger.info(f"‚úÖ WhisperX –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.1f} —Å–µ–∫")
    
    def transcribe(
        self,
        audio_path: Path,
        language: Optional[str] = None,
        diarize: bool = True,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π.
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            diarize: –í—ã–ø–æ–ª–Ω—è—Ç—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤
            min_speakers: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (hint)
            max_speakers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–ø–∏–∫–µ—Ä–æ–≤ (hint)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏:
            - text: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            - segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å speaker
            - language: –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —è–∑—ã–∫
            - speakers: —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ø–∏–∫–µ—Ä—ã
        """
        self.load_model()
        
        audio_str = str(audio_path)
        
        # === –®–∞–≥ 1: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ===
        logger.info("üìù –®–∞–≥ 1/3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è...")
        print("üìù –®–∞–≥ 1/3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è...")
        
        t0 = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        audio = whisperx.load_audio(audio_str)
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
        result = self.model.transcribe(
            audio,
            batch_size=Config.WHISPERX_BATCH_SIZE,
            language=language
        )
        
        detected_language = result.get("language", language or "ru")
        logger.info(f"–Ø–∑—ã–∫: {detected_language}, —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(result['segments'])}")
        
        transcribe_time = time.time() - t0
        logger.debug(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–Ω—è–ª–∞ {transcribe_time:.1f} —Å–µ–∫")
        
        # === –®–∞–≥ 2: Forced Alignment (—Ç–æ—á–Ω—ã–µ timestamps) ===
        logger.info("‚è±Ô∏è  –®–∞–≥ 2/3: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ timestamps...")
        print("‚è±Ô∏è  –®–∞–≥ 2/3: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ timestamps...")
        
        t1 = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (–∫—ç—à–∏—Ä—É–µ–º)
        if self._align_language != detected_language:
            logger.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª—è {detected_language}")
            self._align_model, self._align_metadata = whisperx.load_align_model(
                language_code=detected_language,
                device=self.device
            )
            self._align_language = detected_language
        
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º
        result = whisperx.align(
            result["segments"],
            self._align_model,
            self._align_metadata,
            audio,
            self.device,
            return_char_alignments=False
        )
        
        align_time = time.time() - t1
        logger.debug(f"–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∑–∞–Ω—è–ª–æ {align_time:.1f} —Å–µ–∫")
        
        # === –®–∞–≥ 3: –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ===
        speakers_found = set()
        
        if diarize:
            if not self.hf_token:
                logger.warning(
                    "‚ö†Ô∏è HF_TOKEN –Ω–µ –∑–∞–¥–∞–Ω, –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞. "
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ env HF_TOKEN —Å —Ç–æ–∫–µ–Ω–æ–º HuggingFace."
                )
                print("‚ö†Ô∏è –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–Ω—É–∂–µ–Ω HF_TOKEN)")
            else:
                logger.info("üé≠ –®–∞–≥ 3/3: –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤...")
                print("üé≠ –®–∞–≥ 3/3: –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤...")
                
                t2 = time.time()
                
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º pipeline –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
                    from whisperx.diarize import DiarizationPipeline
                    diarize_model = DiarizationPipeline(
                        use_auth_token=self.hf_token,
                        device=self.device
                    )
                    
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
                    diarize_kwargs = {}
                    if min_speakers is not None:
                        diarize_kwargs["min_speakers"] = min_speakers
                    if max_speakers is not None:
                        diarize_kwargs["max_speakers"] = max_speakers
                    
                    diarize_segments = diarize_model(
                        audio,
                        **diarize_kwargs
                    )
                    
                    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞–º
                    result = whisperx.assign_word_speakers(
                        diarize_segments,
                        result
                    )
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
                    for seg in result.get("segments", []):
                        if "speaker" in seg:
                            speakers_found.add(seg["speaker"])
                    
                    diarize_time = time.time() - t2
                    logger.info(
                        f"‚úÖ –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(speakers_found)} —Å–ø–∏–∫–µ—Ä(–æ–≤), "
                        f"{diarize_time:.1f} —Å–µ–∫"
                    )
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(speakers_found)}")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}", exc_info=True)
                    print(f"‚ö†Ô∏è –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        else:
            logger.info("–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (--no-diarize)")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        segments = result.get("segments", [])
        full_text = " ".join(seg.get("text", "").strip() for seg in segments)
        
        total_time = time.time() - t0
        logger.info(
            f"üìä WhisperX –∑–∞–≤–µ—Ä—à—ë–Ω: {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, "
            f"{len(full_text.split())} —Å–ª–æ–≤, {total_time:.1f} —Å–µ–∫"
        )
        
        return {
            "text": full_text,
            "segments": segments,
            "language": detected_language,
            "speakers": sorted(speakers_found) if speakers_found else []
        }
    
    def format_segments_with_speakers(
        self,
        segments: List[Dict],
        include_timestamps: bool = True
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∫–∞–º–∏ —Å–ø–∏–∫–µ—Ä–æ–≤.
        
        Args:
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            include_timestamps: –í–∫–ª—é—á–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        lines = []
        current_speaker = None
        current_text = []
        current_start = None
        
        for seg in segments:
            speaker = seg.get("speaker", "UNKNOWN")
            text = seg.get("text", "").strip()
            start = seg.get("start", 0)
            
            if not text:
                continue
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            if speaker == current_speaker:
                current_text.append(text)
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –≥—Ä—É–ø–ø—É
                if current_text:
                    combined = " ".join(current_text)
                    if include_timestamps:
                        ts = self._format_time(current_start)
                        lines.append(f"[{ts}] {current_speaker}: {combined}")
                    else:
                        lines.append(f"{current_speaker}: {combined}")
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é –≥—Ä—É–ø–ø—É
                current_speaker = speaker
                current_text = [text]
                current_start = start
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥—Ä—É–ø–ø—É
        if current_text:
            combined = " ".join(current_text)
            if include_timestamps:
                ts = self._format_time(current_start)
                lines.append(f"[{ts}] {current_speaker}: {combined}")
            else:
                lines.append(f"{current_speaker}: {combined}")
        
        return "\n\n".join(lines)
    
    @staticmethod
    def _format_time(seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –≤ MM:SS."""
        minutes = int(seconds) // 60
        secs = int(seconds) % 60
        return f"{minutes:02d}:{secs:02d}"


def check_whisperx_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å WhisperX."""
    return HAS_WHISPERX


def check_diarization_ready() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ª–∏—á–∏–µ HF —Ç–æ–∫–µ–Ω–∞)."""
    return bool(Config.HF_TOKEN)

